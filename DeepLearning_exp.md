## DeepLearning Exp

### 梯度消失、爆炸
  网络层数越多越容易发生,在前几层最明显，因为反向传播时候，最前几层是链式法则叠加最多的位置
  反向传播需要乘的项：loss的导数，一堆激活函数的导数与W的乘积项（权重W一般以），某个输入项
  不同激活函数导数不同，例如sigmoid'，介于0-0.25，而W一般以标准正态分布初始化，所以W行列
  式<1，所以激活函数导数和W乘积项小于0.25
  有时候乘积项由于W被训练而改变导致>1，造成梯度爆炸
  梯度消失更容易发生，因为当W很大时，WX+b可能也大，所以sigmoid的导数会很小，导致梯度消失
  sigmoid经常梯度消失
  relu不会产生由于激活函数造成的梯度消失，无法解决梯度爆炸问题
  relu不会一直死亡，当输入改变的时候，可能复活，在第一个隐藏层的relu可能永久死亡

  解决梯度消失：采用relu等激活函数、残差网络、BatchNormalization、He/Xavier初始化


### 激活函数作用，比较
  特性：非线性、单调递增，大多位置可导
  作用：引入非线性，不然深度网络还是线性的
  希望的特点：零对称、无饱和区（饱和区导数很小，避免梯度消失）、梯度计算高效
  零对称：对于一个neuron来说连接它的所有W都跟上一层的输出有关，如果上一层输出都是正值（比如说上一层
      以sigmoid或者relu作为激活函数，那么这些W的梯度只能都为正，或者都为负，会造成优化缓慢，锯齿形）
  Sigmoid和tanh的缺点，无法提升模型的稀疏性，很多节点输出接近0但是节点仍然存在；梯度消失问题
  relu优点：稀疏性、好计算、不容易梯度消失
  relu缺点：无上限、由于没有很好的初始化以及batchnorm，造成的dyingrelu


### 输入标准化：
  求出训练集X_train的均值和方差，将X_train减去均值再除以方差，可以加速梯度下降，训练完
  test的时候也需要将X_test减去刚刚算出的X_train的均值和除以方差，势必要确定训练集和测
  试集用了同一套均值方差进行缩放。可以加速梯度下降

### 正则化
  防止过拟合和梯度爆炸

### 基于梯度下降的优化算法
  理想方式：牛顿法，是二阶优化，可以求当前梯度，以及梯度的变化趋势，但是需要计算海森矩阵，涉及到二阶导数，
  由于神经网络参数太多，海森矩阵几乎无法计算，所以退而求其次，求梯度并且估计梯度的变化趋势

  Momentum:在山谷地带，存在在某个方向梯度大，另一个方向梯度摆来摆去的问题，在本次位置更新过程中，
    考虑上一次的速度（一阶动量），结果就是摇摆的部分被抑制，保持方向的地方会促进。缺点是会过冲，需要走回头路。

  AdaGrad:可以给不同的参数赋予不同的学习率，过往变化很剧烈的参数在未来允许较低的变化，过往弱变化参数
    在未来加强其变化，可以使稀疏参数较快被优化。问题在于梯度平方和越来越大，会导致所有的学习率都变低。

  RMSProp:跟AdaGrad类似，但是没有采用计算所有过往的梯度平方和，而是指数加权平均平方和（二阶动量），同样试图抑制
  摇摆，并且给比较小被更新的参数更大的机会（更新速度）。

  Adam:结合了Momentum和RMSProp，获得了Momentum的速度，并且可以调节各个参数的学习速率

### 关于Bias和Variance
  先确定贝叶斯误差eBayers，在自然感知问题通常以人类分辨率作为贝叶斯误差的代理，接着计算训练集误差eTrain和验证集误差eDev/eVal，
  当eTrain、eVal和eBayers差不多时说明可避免bias和可避免Variance都已经很小了，模型很成功
  eTrain-eBayers比较大，证明欠拟合
    解决方法：更大（复杂）的网络、更长的训练时间、更好的优化算法、其他神经网络架构、超参数搜索
  eTrain接近eBayers，但是eVal-eTrain比较大，说明Variance很高，过拟合
    解决方法：更多数据（数据扩增）、正则化（包括L2正则化、Dropout）、early stopping、其他神经网络架构、超参数搜索
